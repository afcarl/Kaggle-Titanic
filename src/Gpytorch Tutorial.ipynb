{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T22:25:28.682026Z",
     "start_time": "2019-01-10T22:25:28.680370Z"
    }
   },
   "source": [
    "# $GPy$*Torch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T22:26:15.869179Z",
     "start_time": "2019-01-10T22:26:15.187878Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as tc\n",
    "import gpytorch as gpy\n",
    "\n",
    "#configure plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config IPCompleter.greedy = True\n",
    "%config IPCompleter.use_jedi = True\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16,9)\n",
    "matplotlib.rcParams['font.size'] = 24\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most GP regression models, you will need to construct the following GPyTorch objects:\n",
    "\n",
    "* A **GP Model** (`gpytorch.models.ExactGP`) - This handles most of the inference.\n",
    "* A **Likelihood** (`gpytorch.likelihoods.GaussianLikelihood`) - This is the most common likelihood used for GP regression.\n",
    "* A **Mean** - This defines the prior mean of the GP.\n",
    "\n",
    "    * If you don’t know which mean to use, a `gpytorch.means.ConstantMean()` is a good place to start.\n",
    "\n",
    "* A **Kernel** - This defines the prior covariance of the GP.\n",
    "\n",
    "    * If you don’t know which kernel to use, a `gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())` is a good place to start.\n",
    "\n",
    "* A **MultivariateNormal Distribution** (`gpytorch.distributions.MultivariateNormal`) - This is the object used to represent multivariate normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The GP Model**\n",
    "\n",
    "* For setting up the model, we need to implement a `__init__` that can take training \n",
    "data and a likelihood, and construct for the `forward` method. \n",
    "* The `forward` method takes some $nxd$ data $X$ and returns a **MultiVariateNormal** with prior mean and covariance evaluated at $X$. i.e. a vector $mu(x)$ and the $nxn$ matrix $K_{xx}$ representing the prior mean and covariance matrices of the GP.\n",
    "\n",
    "* E.g. , Adding kernel modulesis valid in both these ways.\n",
    "\n",
    "    ```python\n",
    "    self.covar_module = ScaleKernel(RBFKernel() + WhiteNoiseKernel())\n",
    "    covar_x = self.rbf_kernel_module(x) + self.white_noise_module(x)\n",
    "\n",
    "    ```\n",
    "    \n",
    "* Example:\n",
    "    \n",
    "```python\n",
    "        \n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "```\n",
    "\n",
    "**Model modes**\n",
    "\n",
    "* Like most PyTorch modules, the `ExactGP` has a `.train()` and `.eval()` mode. - `.train()` mode is for optimizing model hyperameters. - `.eval()` mode is for computing predictions through the model posterior.\n",
    "\n",
    "**Training the model**\n",
    "\n",
    "In the next cell, we handle using Type-II MLE to train the hyperparameters of the Gaussian process.\n",
    "\n",
    "The most obvious difference here compared to many other GP implementations is that, as in standard PyTorch, the core training loop is written by the user. In GPyTorch, we make use of the standard PyTorch optimizers as from `torch.optim`, and all trainable parameters of the model should be of type `torch.nn.Parameter`. Because GP models directly extend `torch.nn.Module`, calls to methods like `model.parameters()` or `model.named_parameters()` function as you might expect coming from PyTorch.\n",
    "\n",
    "In most cases, the boilerplate code below will work well. It has the same basic components as the standard PyTorch training loop:\n",
    "\n",
    "    1. Zero all parameter gradients\n",
    "    2. Call the model and compute the loss\n",
    "    3. Call backward on the loss to fill in gradients\n",
    "    4. Take a step on the optimizer\n",
    "\n",
    "However, defining custom training loops allows for greater flexibility. For example, it is easy to save the parameters at each step of training, or use different learning rates for different parameters (which may be useful in deep kernel learning for example).\n",
    "\n",
    "```python\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()},], lr=0.1)\n",
    "# Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f log_lengthscale: %.3f log_noise: %.3f' %\n",
    "         (\n",
    "             i + 1, training_iter, loss.item(),\n",
    "             model.covar_module.base_kernel.log_lengthscale.item(),\n",
    "             model.likelihood.log_noise.item()\n",
    "         ))\n",
    "    optimizer.step()\n",
    "```\n",
    "```output\n",
    "Iter 1/50 - Loss: 1.084   log_lengthscale: 0.000   log_noise: 0.000\n",
    "Iter 2/50 - Loss: 1.043   log_lengthscale: -0.100   log_noise: -0.100\n",
    "Iter 3/50 - Loss: 1.004   log_lengthscale: -0.196   log_noise: -0.200\n",
    "Iter 4/50 - Loss: 0.964   log_lengthscale: -0.293   log_noise: -0.300\n",
    "Iter 5/50 - Loss: 0.922   log_lengthscale: -0.387   log_noise: -0.399\n",
    "Iter 6/50 - Loss: 0.877   log_lengthscale: -0.479   log_noise: -0.499\n",
    "Iter 7/50 - Loss: 0.825   log_lengthscale: -0.572   log_noise: -0.598\n",
    "Iter 8/50 - Loss: 0.767   log_lengthscale: -0.667   log_noise: -0.698\n",
    "Iter 9/50 - Loss: 0.705   log_lengthscale: -0.762   log_noise: -0.799\n",
    "Iter 10/50 - Loss: 0.644   log_lengthscale: -0.860   log_noise: -0.899\n",
    "Iter 11/50 - Loss: 0.590   log_lengthscale: -0.960   log_noise: -1.001\n",
    "Iter 12/50 - Loss: 0.543   log_lengthscale: -1.058   log_noise: -1.102\n",
    "Iter 13/50 - Loss: 0.502   log_lengthscale: -1.150   log_noise: -1.204\n",
    "Iter 14/50 - Loss: 0.462   log_lengthscale: -1.234   log_noise: -1.306\n",
    "Iter 15/50 - Loss: 0.426   log_lengthscale: -1.303   log_noise: -1.408\n",
    "Iter 16/50 - Loss: 0.389   log_lengthscale: -1.360   log_noise: -1.509\n",
    "Iter 17/50 - Loss: 0.360   log_lengthscale: -1.404   log_noise: -1.611\n",
    "Iter 18/50 - Loss: 0.321   log_lengthscale: -1.432   log_noise: -1.712\n",
    "Iter 19/50 - Loss: 0.280   log_lengthscale: -1.454   log_noise: -1.812\n",
    "Iter 20/50 - Loss: 0.250   log_lengthscale: -1.465   log_noise: -1.911\n",
    "Iter 21/50 - Loss: 0.227   log_lengthscale: -1.469   log_noise: -2.010\n",
    "Iter 22/50 - Loss: 0.188   log_lengthscale: -1.461   log_noise: -2.108\n",
    "Iter 23/50 - Loss: 0.158   log_lengthscale: -1.442   log_noise: -2.204\n",
    "Iter 24/50 - Loss: 0.125   log_lengthscale: -1.411   log_noise: -2.300\n",
    "Iter 25/50 - Loss: 0.095   log_lengthscale: -1.377   log_noise: -2.393\n",
    "Iter 26/50 - Loss: 0.070   log_lengthscale: -1.340   log_noise: -2.485\n",
    "Iter 27/50 - Loss: 0.050   log_lengthscale: -1.298   log_noise: -2.574\n",
    "Iter 28/50 - Loss: 0.032   log_lengthscale: -1.256   log_noise: -2.662\n",
    "Iter 29/50 - Loss: 0.014   log_lengthscale: -1.218   log_noise: -2.746\n",
    "Iter 30/50 - Loss: 0.003   log_lengthscale: -1.182   log_noise: -2.828\n",
    "Iter 31/50 - Loss: -0.001   log_lengthscale: -1.148   log_noise: -2.906\n",
    "Iter 32/50 - Loss: -0.008   log_lengthscale: -1.121   log_noise: -2.980\n",
    "Iter 33/50 - Loss: -0.012   log_lengthscale: -1.102   log_noise: -3.049\n",
    "Iter 34/50 - Loss: -0.011   log_lengthscale: -1.103   log_noise: -3.114\n",
    "Iter 35/50 - Loss: -0.014   log_lengthscale: -1.114   log_noise: -3.174\n",
    "Iter 36/50 - Loss: -0.014   log_lengthscale: -1.138   log_noise: -3.228\n",
    "Iter 37/50 - Loss: -0.010   log_lengthscale: -1.169   log_noise: -3.275\n",
    "Iter 38/50 - Loss: -0.011   log_lengthscale: -1.204   log_noise: -3.317\n",
    "Iter 39/50 - Loss: -0.008   log_lengthscale: -1.239   log_noise: -3.352\n",
    "Iter 40/50 - Loss: -0.001   log_lengthscale: -1.270   log_noise: -3.380\n",
    "Iter 41/50 - Loss: -0.005   log_lengthscale: -1.296   log_noise: -3.401\n",
    "Iter 42/50 - Loss: 0.008   log_lengthscale: -1.317   log_noise: -3.415\n",
    "Iter 43/50 - Loss: 0.001   log_lengthscale: -1.331   log_noise: -3.422\n",
    "Iter 44/50 - Loss: 0.009   log_lengthscale: -1.343   log_noise: -3.423\n",
    "Iter 45/50 - Loss: 0.001   log_lengthscale: -1.350   log_noise: -3.419\n",
    "Iter 46/50 - Loss: -0.001   log_lengthscale: -1.360   log_noise: -3.410\n",
    "Iter 47/50 - Loss: 0.007   log_lengthscale: -1.374   log_noise: -3.397\n",
    "Iter 48/50 - Loss: 0.000   log_lengthscale: -1.388   log_noise: -3.380\n",
    "Iter 49/50 - Loss: -0.010   log_lengthscale: -1.396   log_noise: -3.359\n",
    "Iter 50/50 - Loss: -0.008   log_lengthscale: -1.404   log_noise: -3.337\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions with the model**\n",
    "\n",
    "* In the next cell, we make predictions with the model. To do this, we simply put the model and likelihood in `.eval()` mode, and call both modules on the test data.\n",
    "\n",
    "* Just as a user defined GP model returns a `**MultivariateNormal**` containing the prior mean and covariance from `forward`, a trained GP model in `eval` mode returns a `**MultivariateNormal**` containing the posterior mean and covariance. Thus, getting the predictive mean and variance, and then sampling functions from the GP at the given test points could be accomplished with calls like:\n",
    "\n",
    "```python\n",
    "f_preds = model(test_x)\n",
    "y_preds = likelihood(model(test_x))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n",
    "```\n",
    "\n",
    "*The `gpytorch.settings.fast_pred_var` context is not needed, but here we are giving a preview of using one of our cool features, getting faster predictive distributions using [LOVE](https://arxiv.org/abs/1803.06058).\n",
    "\n",
    "```python\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "```\n",
    "\n",
    "**Plot the model fit**\n",
    "\n",
    "In the next cell, we plot the mean and confidence region of the Gaussian process model. The `confidence_region` method is a helper method that returns 2 standard deviations above and below the mean.\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "```\n",
    "![Plot](https://gpytorch.readthedocs.io/en/latest/_images/examples_01_Simple_GP_Regression_Simple_GP_Regression_12_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most variational GP models, you will need to construct the following GPyTorch objects:\n",
    "\n",
    "* A **GP Model** (`gpytorch.models.AbstractVariationalGP`) - This handles basic variational inference.\n",
    "* A **Variational distribution** (`gpytorch.variational.VariationalDistribution`) - This tells us what form the variational distribution $q(u)$ should take.\n",
    "* A **Variational strategy** (`gpytorch.variational.VariationalStrategy`) - This tells us how to transform a distribution $q(u)$ over the inducing point values to a distribution $q(f)$ over the latent function values for some input $x$.\n",
    "* A **Likelihood** (`gpytorch.likelihoods.BernoulliLikelihood`) - This is a good likelihood for *binary classification*\n",
    "* A **Mean** - This defines the prior mean of the GP.\n",
    "\n",
    "    * If you don’t know which mean to use, a `gpytorch.means.ConstantMean()` is a good place to start.\n",
    "\n",
    "* A **Kernel** - This defines the prior covariance of the GP.\n",
    "\n",
    "    * If you don’t know which kernel to use, use a `gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())` is a good place to start.\n",
    "\n",
    "* A **MultivariateNormal Distribution** (`gpytorch.distributions.MultivariateNormal`) - This\n",
    "\n",
    "**The GP Model**\n",
    "\n",
    "The `AbstractVariationalGP` model is GPyTorch’s simplist approximate inference model. It approximates the true posterior with a distribution specified by a `VariationalDistribution`, which is most commonly some form of `MultivariateNormal` distribution. The model defines all the variational parameters that are needed, and keeps all of this information under the hood.\n",
    "\n",
    "The components of a user built `AbstractVariationalGP` model in GPyTorch are:\n",
    "\n",
    "    1. An `__init__` method that constructs a mean module, a kernel module, a variational distribution object and a variational strategy object. This method should also be responsible for construting whatever other modules might be necessary.\n",
    "    2. A forward method that takes in some $n×d$ data $x$ and returns a `MultivariateNormal` with the prior mean and covariance evaluated at $x$. In other words, we return the vector $μ(x)$ and the $n×n$ matrix $K_{xx}$ representing the prior mean and covariance matrix of the GP.\n",
    "\n",
    "(For those who are unfamiliar with GP classification: even though we are performing classification, the GP model still returns a `MultivariateNormal`. The likelihood transforms this latent Gaussian variable into a Bernoulli variable)\n",
    "\n",
    "Here we present a simple classification model, but it is possible to construct more complex models. See some of the scalable classification examples or deep kernel learning examples for some other examples.\n",
    "\n",
    "```python\n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPClassificationModel(AbstractVariationalGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "\n",
    "# Initialize model and likelihood\n",
    "model = GPClassificationModel(train_x)\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "```\n",
    "\n",
    "**Model Modes**\n",
    "\n",
    "Like most PyTorch modules, the `ExactGP` has a `.train()` and `.eval()` mode. - `.train()` mode is for optimizing variational parameters model hyperameters. - `.eval()` mode is for computing predictions through the model posterior.\n",
    "\n",
    "**Learn the variational parameters (and other hyperparameters)**\n",
    "\n",
    "In the next cell, we optimize the variational parameters of our Gaussian process. In addition, this optimization loop also performs Type-II MLE to train the hyperparameters of the Gaussian process.\n",
    "\n",
    "The most obvious difference here compared to many other GP implementations is that, as in standard PyTorch, the core training loop is written by the user. In GPyTorch, we make use of the standard PyTorch optimizers as from `torch.optim`, and all trainable parameters of the model should be of type `torch.nn.Parameter`. The variational parameters are predefined as part of the `VariationalGP` model.\n",
    "\n",
    "In most cases, the boilerplate code below will work well. It has the same basic components as the standard PyTorch training loop:\n",
    "\n",
    "    1. Zero all parameter gradients\n",
    "    2. Call the model and compute the loss\n",
    "    3. Call backward on the loss to fill in gradients\n",
    "    4. Take a step on the optimizer\n",
    "\n",
    "However, defining custom training loops allows for greater flexibility. For example, it is possible to learn the variational parameters and kernel hyperparameters with different learning rates.\n",
    "\n",
    "```python\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the amount of training data\n",
    "mll = VariationalELBO(likelihood, model, train_y.numel())\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "```output\n",
    "Iter 1/50 - Loss: 1.225\n",
    "Iter 2/50 - Loss: 8.492\n",
    "Iter 3/50 - Loss: 2.531\n",
    "Iter 4/50 - Loss: 3.006\n",
    "Iter 5/50 - Loss: 5.014\n",
    "Iter 6/50 - Loss: 3.859\n",
    "Iter 7/50 - Loss: 1.783\n",
    "Iter 8/50 - Loss: 1.525\n",
    "Iter 9/50 - Loss: 2.158\n",
    "Iter 10/50 - Loss: 2.525\n",
    "Iter 11/50 - Loss: 2.080\n",
    "Iter 12/50 - Loss: 1.602\n",
    "Iter 13/50 - Loss: 1.520\n",
    "Iter 14/50 - Loss: 1.704\n",
    "Iter 15/50 - Loss: 1.773\n",
    "Iter 16/50 - Loss: 1.597\n",
    "Iter 17/50 - Loss: 1.438\n",
    "Iter 18/50 - Loss: 1.574\n",
    "Iter 19/50 - Loss: 1.795\n",
    "Iter 20/50 - Loss: 1.737\n",
    "Iter 21/50 - Loss: 1.847\n",
    "Iter 22/50 - Loss: 1.789\n",
    "Iter 23/50 - Loss: 1.505\n",
    "Iter 24/50 - Loss: 1.369\n",
    "Iter 25/50 - Loss: 1.503\n",
    "Iter 26/50 - Loss: 1.363\n",
    "Iter 27/50 - Loss: 1.322\n",
    "Iter 28/50 - Loss: 1.330\n",
    "Iter 29/50 - Loss: 1.378\n",
    "Iter 30/50 - Loss: 1.343\n",
    "Iter 31/50 - Loss: 1.416\n",
    "Iter 32/50 - Loss: 1.467\n",
    "Iter 33/50 - Loss: 1.441\n",
    "Iter 34/50 - Loss: 1.425\n",
    "Iter 35/50 - Loss: 1.327\n",
    "Iter 36/50 - Loss: 1.498\n",
    "Iter 37/50 - Loss: 1.393\n",
    "Iter 38/50 - Loss: 1.208\n",
    "Iter 39/50 - Loss: 1.429\n",
    "Iter 40/50 - Loss: 1.361\n",
    "Iter 41/50 - Loss: 1.435\n",
    "Iter 42/50 - Loss: 1.287\n",
    "Iter 43/50 - Loss: 1.673\n",
    "Iter 44/50 - Loss: 1.601\n",
    "Iter 45/50 - Loss: 1.275\n",
    "Iter 46/50 - Loss: 1.321\n",
    "Iter 47/50 - Loss: 1.750\n",
    "Iter 48/50 - Loss: 1.487\n",
    "Iter 49/50 - Loss: 1.195\n",
    "Iter 50/50 - Loss: 1.430\n",
    "```\n",
    "\n",
    "**Make predictions with the model**\n",
    "\n",
    "In the next cell, we make predictions with the model. To do this, we simply put the model and likelihood in eval mode, and call both modules on the test data.\n",
    "\n",
    "In `.eval()` mode, when we call `model()` - we get GP’s latent posterior predictions. These will be `MultivariateNormal` distributions. But since we are performing binary classification, we want to transform these outputs to classification probabilities using our likelihood.\n",
    "\n",
    "When we call `likelihood(model())`, we get a `torch.distributions.Bernoulli` distribution, which represents our posterior probability that the data points belong to the positive class.\n",
    "\n",
    "```python\n",
    "f_preds = model(test_x)\n",
    "y_preds = likelihood(model(test_x))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size((1000,))\n",
    "```\n",
    "\n",
    "```python\n",
    "# Go into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test x are regularly spaced by 0.01 0,1 inclusive\n",
    "    test_x = torch.linspace(0, 1, 101)\n",
    "    # Get classification predictions\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    # Initialize fig and axes for plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Get the predicted labels (probabilites of belonging to the positive class)\n",
    "    # Transform these probabilities to be 0/1 labels\n",
    "    pred_labels = observed_pred.mean.ge(0.5).float().mul(2).sub(1)\n",
    "    ax.plot(test_x.numpy(), pred_labels.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "```\n",
    "![Plot](https://gpytorch.readthedocs.io/en/latest/_images/examples_02_Simple_GP_Classification_Simple_GP_Classification_10_0.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
